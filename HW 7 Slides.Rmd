---
title: "The Exponential Distribution"
subtitle: "Estimators and Confidence Intervals"
author: "Abhijith Asok, Chris Hilger, Liam Loscalzo, Katherine Wang"
output: beamer_presentation
header-includes:
   - \usepackage{makecell} 
theme: "Boadilla" 
##colortheme: "structure"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(gridExtra)
set.seed(123)
```

# Point estimator selection and comparison

We compared the point estimators listed on the following slide by looking at plots of mean squared error (MSE) and estimator properties like consistency, bias, and asymptotic bias.

The first three estimators were chosen before looking at the MSE plot. We know that the MLE and unbaised correction to the MLE have some desirable characteristics. We also chose to investigate a method of moments estimator. Since the first moment $E[X] = \frac{n}{\sum{x}_i}$ is the same as the MLE, we looked at the second moment instead:  $E[X^2] = 2 * \bar{x}^2$

The last two estimators were developed in an attempt to achieve a lower MSE for different values of $\lambda$ and $n$ ($4 <\lambda < 6$ and $n = 10, 25$, respectively)

## Exponential parameterization
$$
f(x, \lambda) = \lambda e^{-\lambda x}
$$

# The chosen estimators

\begin{center}
\begin{tabular}{ ||p{2cm}|p{6cm}|p{1cm}||  }
 \hline
 \multicolumn{3}{|c|}{Estimator list} \\
 \hline
 Symbol & Estimator & Value \\
 \hline
 $\hat{\lambda}_0$ & Maximum Likelihood Estimator & $\frac{n}{\sum{x}_i}$\\
 \hline
 $\hat{\lambda}_1$ & Unbiased correction for the MLE & $\frac{n-1}{\sum{x}_i}$\\
 \hline
 $\hat{\lambda}_2$ & Second Method of Moment Estimator & $2 \bar{x}^2$\\
 \hline
 $\hat{\lambda}_3$ & Hard estimate, centered around 5 & $5$\\
 \hline
 $\hat{\lambda}_4$ & Similar to the MLE, without $n$ in numerator  & $\frac{1}{\sum{x}_i}$\\
 \hline
\end{tabular}
\end{center}

# Maximum likelihood estimator: $\hat{\lambda}_0$ 

Given how we parameterized the exponential distribution, we cannot easily take the expected value of $\hat{\lambda}_0$ since $\sum{x}_i$ is in the denominator. However, we recognize that $\sum{x}_i$ for an exponential variable is gamma-distributed. To make things easier, 

## Expectation calculation

Let $y = \sum{x}_i$ and $E[ \hat{\lambda}_0 ] = E\left[\frac{n}{\sum{x}_i}\right] = E[\frac{n}{y}]$

So:

$$
E\left[\frac{n}{y}\right] = \int_{0}^{\infty}\frac{1}{(n-1)!\lambda^n} x^{(n-1)}e^{\left(\frac{-x}{\lambda}\right)}dy
$$

Computing this integral yields:  

$$
E[\hat{\lambda}_0] = \lambda \left(\frac{n}{n-1}\right)
$$

# Maximum likelihood estimator: $\hat{\lambda}_0$ 

## Variance calculation

We need to calculate $\text{Var}[\hat{\lambda}_0]$  

Here we will use the expected information $I = -l''({\lambda}_0)$

$$
I = - E[l''({\lambda}_0)] = - E\left[- \frac{\lambda^2}{n}\right]
$$

and we know $\text{Var}[\theta] = \frac{1}{I[\theta]}$

Therefore:

$$
\text{Var}[\hat{\lambda}_0] = \frac{\lambda^2}{n}
$$

# Maximum likelihood estimator: $\hat{\lambda}_0$ 

## Calculating the bias

\begin{align*}
Bias &= E[\hat{\lambda}_0 - \lambda]\\
&= \lambda \left(\frac{n}{n-1}\right) - \lambda
\end{align*}

## Calculating the MSE

\begin{align*}
MSE &= \text{Bias}^2 + \text{Var} =  (E[{\lambda}_0 - \lambda])^2 + \text{Var}([\hat{\lambda}_0])\\
&= \left(\lambda\frac{n}{n-1} - \lambda\right)^2 + \frac{\lambda^2}{n}
\end{align*}

# Classifying the estimators

**Unbiased**: No, since the $E[\hat{\lambda}_0] \neq \lambda$  
**Asymptotically unbiased**: Yes, since the $\lim_{n\to\infty} \lambda \left(\frac{n}{n-1}\right)$ equals $\lambda$  
**Consistent**: Yes, since it is Asymptotically Unbiased and $\lim_{n\to\infty} \frac{\lambda^2}{n}$ equals $0$

We calculate values for each of the estimators to construct the table below:

\begin{tabular}{ |p{2cm}||p{1cm}|p{1cm}|p{1cm}| p{1.5cm}|p{3cm}|}
 \hline
 \multicolumn{6}{|c|}{Values for each estimator} \\
 \hline
 Estimator&Value& $E[\lambda]$&$Var[\lambda]$& $Bias[\lambda]$&$MSE[\lambda]$\\ [7pt]
 \hline
 $\hat{\lambda}_0$& $\frac{n}{\sum{x}_i}$  & $\lambda\frac{n}{n-1}$& $\frac{\lambda^2}{n}$   & $\lambda\frac{n}{n-1}-\lambda$  & $(\lambda\frac{n}{n-1} - \lambda)^2 + \frac{\lambda^2}{n}$\\[7pt]
 \hline
 
 $\hat{\lambda}_1$&  $\frac{n-1}{\sum{x}_i}$ & $\lambda$  & $\frac{\lambda^2}{n}\frac{n-1}{n}^2$  & $0$& $\frac{\lambda^2}{n}\frac{n-1}{n}^2$\\[7pt]
 \hline
 
 
 $\hat{\lambda}_2$&   $2 \bar{x}^2$  & $\frac{2}{\lambda^2}$ & $\frac{16}{n\lambda^4}$ & $\frac{2}{\lambda^2}-\lambda$ & $({\frac{2}{\lambda^2}-\lambda)}^2 - \frac{16}{n\lambda^4}$ \\[7pt]
 \hline
 
 
 $\hat{\lambda}_3$& $5$  & $5$ & $0$ &  $5-\lambda$ & $(5-\lambda)^2$\\[7pt]
 \hline
 
 $\hat{\lambda}_4$&  $\frac{1}{\sum{x}_i}$ & $\frac{\lambda}{n-1}$  & $\frac{\lambda^2}{n^3}$ & $\frac{\lambda}{n-1} - \lambda$ & $ ({\frac{\lambda}{n-1} - \lambda})^2 +\frac{\lambda^2}{n^3}  $ \\[7pt]

 \hline
\end{tabular}

# Properties for all astimators

\begin{tabular}{ |p{2cm}||p{1cm}|p{2cm}|p{2cm}|p{3cm}|}
 \hline
 \multicolumn{5}{|c|}{Properties for Each Estimator} \\
 \hline
 Estimator& Value & Unbiased & \makecell{Asymp.\\ Unbaised} &Consistent\\
 \hline
 $\hat{\lambda}_0$& $\frac{n}{\sum{x}_i}$ & No    & Yes&   Yes\\[7pt]
 \hline
 $\hat{\lambda}_1$& $\frac{n-1}{\sum{x}_i}$ &   Yes  & Yes   &Yes\\[7pt]
 \hline
 $\hat{\lambda}_2$&  $2 \bar{x}^2$ & No & No &  No\\[7pt]
 \hline
 $\hat{\lambda}_3$& $5$ & \makecell{No, \\ unless $\hat{\lambda} = 5$} & No&  No\\[7pt]
 \hline
 $\hat{\lambda}_4$& $\frac{1}{\sum{x}_i}$ &   No  & Yes & Yes\\[7pt]

 \hline
\end{tabular}

# Plotting the MSE

- Now that we have the MSE calculations for all estimators, we can plot them for various $n$ over a range of $\lambda$ values. Specifically, we look at $n = 10$ and $n= 25$. 

- Looking at the MSE for $\hat{\lambda}_1$, which is the unbiased version of the MLE, we find that the MSE is lowest compared to all other estimates, except for that of $\hat{\lambda}_3$ which equals $5$. 

- As discussed earlier, the last two estimators $\hat{\lambda}_3$ and $\hat{\lambda}_4$ were developed in an attempt to achive a lower MSE for a given range of $\lambda$, specifically $4 <\lambda < 6$  at $n = 10, 25$.

- We find that $\hat{\lambda}_3$ achieves this goal nicely, but $\hat{\lambda}_4$ does not.

- Plotting the estimators for n = 10 and n = 25, we see that the hard $\lambda = 5$ estimate would be less unseful because $\hat{\lambda}_1$, which is the unbiased version of the MLE, has an MSE which is much closer to it than at the $n =10$ level.

# Plotting the MSE

```{r, echo=FALSE}
n<-10

lam <-seq(0,10,0.01)
mse0 <- (lam*n/(n-1))^2 + lam^2/n
mse1 <- ((lam^2)/n)*((n-1)/n)^2
mse2<- (2/(lam^2)-lam)^2 + 16/(n*lam^4)
mse3 <- (5-lam)^2
mse4 <- (lam/(n-1)- lam)^2 + lam^2/n^3

plot(lam,mse0,ylab="MSE",xlab="Lambda",main="Compare MSE (n=10)",type="l",col="magenta",ylim=c(0,5))

lines(lam,mse1,type="l",col="blue",lwd=2)
lines(lam,mse2,type="l",col="red")
lines(lam,mse3,type="l",col="orange")
lines(lam,mse4,type="l",col="pink")

legend(8, 5,legend=c("lam0", "lam1", "lam2", "lam3","lam4"),
       col=c("magenta", "blue", "red", "orange" ,"pink"), lty=1, cex=0.8)
```

# Plotting the MSE

```{r,echo=FALSE}
n<-25

lam <-seq(0,10,0.01)
mse0 <- (lam*n/(n-1))^2 + lam^2/n
mse1 <- ((lam^2)/n)*((n-1)/n)^2
mse2<- (2/(lam^2)-lam)^2 + 16/(n*lam^4)
mse3 <- (5-lam)^2
mse4 <- (lam/(n-1)- lam)^2 + lam^2/n^3

plot(lam,mse0,ylab="MSE",xlab="Lambda",main="Compare MSE (n=25)",type="l",col="magenta",ylim=c(0,5))

lines(lam,mse1,type="l",col="blue",lwd=2)
lines(lam,mse2,type="l",col="red")
lines(lam,mse3,type="l",col="orange")
lines(lam,mse4,type="l",col="pink")

legend(8, 5,legend=c("lam0", "lam1", "lam2", "lam3","lam4"),
       col=c("magenta", "blue", "red", "orange" ,"pink"), lty=1, cex=0.8)
```

# For small samples, MoM estimates decrease, then increase with increasing degree 

```{r, echo=FALSE}
library(gridExtra)
library(tidyverse)

momest_exp <- function(v,k){
  return((factorial(k)/(sum(v^k)/length(v)))^(1/k))
}

k <- 10
moment_number <- seq(1,k,by=1)
sample_size <- c(10,50,100)
par(mfrow = c(2,3))
g <- list()
c <- 1
#plotlabel <- data.frame(text = c("MLE","MLE","MLE","MLE","MLE","Unbiased Correction for MLE", 
#                                 "Unbiased Correction for MLE","Unbiased Correction for MLE",
#                                 "Unbiased Correction for MLE","Unbiased Correction for MLE"), 
#                        x = c(10,10,10,10,10,8,5,8,8,8), 
#                        y = c(0.08,0.0475,0.048,0.05,0.0503,0.0775,0.047,0.048,0.0495,0.05))
for(i in sample_size){
  set.seed(123)
  exp_sample <- rexp(i, rate = 0.05)
  moment_estimations <- data.frame(degree = 1:k, moments = matrix(unlist(lapply(moment_number,momest_exp,v=exp_sample))))
  g[[c]] <- ggplot(data = moment_estimations, aes(x=degree,y=moments)) + 
    geom_point(col = "darkblue") + 
    geom_line(col = "blue") +
    ggtitle(paste0("sample size = ",i)) + 
    geom_hline(yintercept = 1/mean(exp_sample), lty = 2, col="darkred") +
    geom_hline(yintercept = (length(exp_sample) - 1)/sum(exp_sample), lty = 2, col="darkgreen") +
    #             geom_text(data=plotlabel[c,], aes(x,y,label=text), size = 3, col="darkred") +
    #              geom_text(data=plotlabel[c + 5,], aes(x,y,label=text), size = 3, col="darkgreen") +
    scale_colour_manual(name = 'the colour', values =c('black'='black','red'='red'), labels = c('c2','c1'))
  theme_bw()
  c <- c + 1
}


grid.arrange(g[[1]], g[[2]], g[[3]], 
             top = "Moment Estimations V/S Moment degree (Actual rate = 0.05) : Red dashed lines are MLE; Green dashed lines are unbiased MLE")
```

# For a range of sample sizes, the first 10 MoM estimators follow similar patterns

```{r, echo=FALSE}
k <- 10
moment_number <- seq(1,k,by=1)
cols <- c(paste0("k",1:k))
sample_size <- seq(1000,100000,by=1000)
moment_estimations <- data.frame(matrix(0, nrow = length(sample_size), ncol = k + 1))
moment_estimations[,1] <- sample_size
colnames(moment_estimations) <- c("n",cols)
library(dplyr)
c <- 1
for (i in sample_size) {
  set.seed(123)
  exp_sample <- rexp(i, rate = 0.05)
  for (j in moment_number) {
    moment_estimations[c,j+1] <- momest_exp(exp_sample,j)
  }
  c <- c + 1
}

plotlabel <- data.frame(text = c("1","2","3","4","5","6","7","8","9","10"), 
                        x = c(rep(3,10)), 
                        y = c(0.0485,0.049,0.05,0.051,0.052,0.053,0.0545,0.056,0.0575,0.0595))

moment_estimations %>% ggplot(aes(x = n)) + 
  geom_line(aes(y=k1),col="violet") + 
  geom_line(aes(y=k2),col="blue") +
  geom_line(aes(y=k3),col="green") + 
  geom_line(aes(y=k4),col="yellow") + 
  geom_line(aes(y=k5),col="orange") + 
  geom_line(aes(y=k6),col="red") + 
  geom_line(aes(y=k7),col="cyan") + 
  geom_line(aes(y=k8),col="magenta") +
  geom_line(aes(y=k9),col="black") + 
  geom_line(aes(y=k10),col="grey50") + 
  xlab("Sample size") + 
  ylab("Estimator value") + 
  ggtitle("Variation of first 10 moment estimators with sample size(Actual rate = 0.05)") + 
  theme_bw() + 
  geom_text(data=plotlabel, aes(x,y,label=text), size = 3, col="darkred")

```

# Confidence intervals: methods

We compared the following confidence intervals:

- Wald-based confidence interval
- Gamma-based confidence interval
- Score-based confidence interval
- Bootstrap confidence interval

## Comparing confidence intervals

To compare confidence intervals, we are interested in understanding both

- The coverage probability of each confidence interval at a given $\alpha$ and $\lambda$ value, evaluated at different sample sizes
- How large the confidence interval is for a given $\alpha$ level, across different sample sizes and values of $\lambda$

A good confidence interval should be as small as possible while providing a coverage probability corresponding to the chosen $\alpha$ level. We perform our assessments using simulated data.

# Wald-based confidence interval

The "standard" confidence interval based on the CLT, which says that as $n$ gets large, the Wald test statistic
$$
T = \frac{\lambda - \lambda_0}{\text{s.e.}(\lambda)} \stackrel{\text{app}}{\sim} N(0,1)
$$
We reject the null hypothesis when $|T| \ge z_{1 - \alpha/2}$. This can be inverted to obtain the $(1 - \alpha)\cdot100\%$ confidence interval
$$
\hat{\lambda} \pm z_{1 - \alpha/2} \cdot \text{s.e.}(\lambda) 
$$

```{r, echo = FALSE}
wald_ci <- function(N, rate, alpha = 0.05){
  x <- rexp(N, rate = rate)
  x_bar <- mean(x)
  se <- x_bar / sqrt(N)
  ci <- x_bar + c(-1, 1) * qnorm(1 - (alpha / 2))*se
  return(ci)
}
```

# Gamma-based confidence interval

## Relation of exponential distribution to gamma distribution

If $X_1, X_2,...,X_n \stackrel{\text{iid}}{\sim} \text{Exponential}(\lambda)$, then

$$
\sum_i^n x_i \sim \text{Gamma}(n, \lambda) \implies \lambda \bar{x} \sim \text{Gamma}(n, n)
$$

##

Let $g_{y}$ be the $y$th percentile of this distribution. Then we can say:

$$
1 - \alpha = P(g_{\alpha/2} \le \lambda \bar{x} \le g_{1-\alpha/2}) = P(g_{\alpha/2}/\bar{x} \le \lambda \le g_{1-\alpha/2}/\bar{x})
$$

Therefore, a $(1 - \alpha)\cdot100\%$ confidence interval for $\lambda$ is 

$$
(g_{\alpha / 2}/\bar{x},\, g_{1 - \alpha / 2}/\bar{x})
$$

Which can be trivially converted to a confidence interval for the mean $\mu = 1/\lambda$

```{r, echo = FALSE}
gamma_ci <- function(N, rate, alpha = 0.05){
  # Inspiration: https://math.stackexchange.com/questions/1288139/calculate-the-confidence-interval-of-parameter-of-exponential-distribution
  x <- rexp(N, rate = rate)
  x_bar <- mean(x)
  ci_rate <- qgamma(c(alpha / 2, 1 - (alpha / 2)), N, N) / x_bar
  ci_mean <- 1 / ci_rate
  return(c(min(ci_mean), max(ci_mean)))
}
```

# Score-based confidence interval

For moderate to large sample sizes, the following score test statistic is approximately normally distributed

$$
\frac{U(\lambda_o)}{\sqrt{\text{Var}[U(\lambda_o)]}} \stackrel{\text{app}}{\sim} N(0,1)
$$

$(1-\alpha) \cdot 100\%$ confidence interval for the mean can be obtained by solving 

$$
\left|\frac{\frac{n}{\lambda_o}-\sum{x_i}}{\sqrt{n/\lambda^2_o}}\right| = \left|\sqrt{n}(1-\lambda_o\bar{x})\right| \ge z_{1-\alpha/2}
$$
for $\lambda_o$, giving us a confidence interval of

$$
\left(\frac{1}{\bar{x}}(1 - \frac{1}{\sqrt{n}}z_{1-\alpha/2}), \frac{1}{\bar{x}}(1 + \frac{1}{\sqrt{n}}z_{1-\alpha/2}) \right)
$$
```{r, echo = FALSE}
score_ci <- function(N, rate, alpha = 0.05) {
  x <- rexp(N, rate = rate)
  xbar <- mean(x)
  ci_rate <- (1 / xbar)*(1 + c(-1, 1) * qnorm(1 - alpha / 2) / sqrt(N))
  ci_mean <- 1 / ci_rate
  return(c(min(ci_mean), max(ci_mean)))
}
```

# Bootstrap confidence interval

- Empirical method which does not require knowledge of underlying distribution for $X$
- Based on resampling data (**with replacement**) many times to create an empirical distribution $U^*$ which approximates the true (unknown) distribution $U$ 
- We estimate the variation of $\bar{x}$ around the true mean $1/\lambda$ using the variation of $\bar{x}^*$ in bootstrapped samples
- As an empirical method, depends on the original data
  - We expect that $\bar{x}^*$ will approximate $\bar{x}$ well, but no guarantee it will be a good estimate of $\mu = 1/\lambda$
  - Not a problem when we use simulated data

```{r, echo = FALSE}
bootstrap_ci <- function(N, rate, alpha = 0.05){
  # Function to calculate bootstrap CI
  x <- rexp(N, rate = rate)
  x_bar <- mean(x)
  # Number of bootstrap samples
  nb <- 1000
  # Take boostrap samples
  bootstrap_samples <- sample(x, N * nb, replace = TRUE) %>%
    matrix(nrow = N, ncol = nb)
  # Get means of columns 
  means <- colMeans(bootstrap_samples)
  # Get deltas (x* - x)
  deltas <- means - x_bar
  deltas <- sort(deltas)
  # Calculate CIs
  ci <- x_bar - quantile(deltas, probs = c(alpha / 2, 1 - (alpha / 2)))
  return(c(mean(ci), max(ci)))
}
```

# Coverage probabilities: $\alpha = 0.05, \lambda = 1/12$

```{r, echo = FALSE, message = FALSE}
# Coverage probabilities
coverage_probability <- function(N, rate, ci_fun, alpha = 0.05, B = 1000){
  # Match input function to actual function
  fun <- tryCatch(match.fun(ci_fun), 
                  error = function(e) print(paste0("ci_fun: '", ci_fun, "' does not exist")))
  # Expected value
  exp_val <- 1 / rate
  # Calculate B confidence intervals and put in dataframe
  conf_ints <- replicate(B, fun(N, rate)) %>%
    t() %>% data.frame()
  names(conf_ints) <- c("lower", "upper")
  # Calculate hit rates (expected value inside confidence interval)
  conf_ints <- mutate(conf_ints, hit = (exp_val >= lower & exp_val <= upper))
  coverage <- conf_ints$hit %>% mean()
  return(coverage)
}

lambda <- 1 / 12
beta <- 1 / lambda
n <- 60

# Coverage probability plotting
ns <- seq(1, 500)

###################################################
# I have commented out all of these so that we don't 
# accidentally re run if we don't have to. I've saved 
# the data for coverage probabilities so we can read it 
# in without re-running if not necessary.
###################################################
# Caution: bootstrap takes for-ev-er
# bootstrap_cov <- mclapply(ns, coverage_probability, rate = lambda, ci_fun = "bootstrap_ci") %>% unlist()
# gamma_cov <- mclapply(ns, coverage_probability, rate = lambda, ci_fun = "gamma_ci") %>% unlist()
#wald_cov <- sapply(ns, coverage_probability, rate = lambda, ci_fun = "wald_ci") %>% unlist()
#score_cov <- sapply(ns, coverage_probability, rate = lambda, ci_fun = "score_ci") %>% unlist()
#coverage_df <- data.frame(n = ns, bs = bootstrap_cov, gamma = gamma_cov, wald = wald_cov, score = score_cov)
#write_csv(coverage_df, "./data/coverage_probabilities.csv")
###################################################
# If need to read:
coverage_df <- read_csv("./data/coverage_probabilities.csv")
#coverage_df$score <- score_cov
#coverage_df$wald <- wald_cov

s = 1/5
ggplot(coverage_df[1:200, ], aes(x = n, y = bs, color = "Bootstrap")) +
  geom_smooth(span = s, se = FALSE) +
  geom_smooth(aes(x = n, y = gamma, color = "Gamma"), span = s, se = FALSE) +
  geom_smooth(aes(x = n, y = wald, color = "Wald"), span = s, se = FALSE) +
  geom_smooth(aes(x = n, y = score, color = "Score"), span = s, se = FALSE) +
  geom_hline(yintercept = 0.95, linetype = 2) + 
  scale_y_continuous(name = "Coverage probability") +
  scale_x_continuous(name = "N") + 
  theme_bw() + 
  theme(legend.position = c(0.9, 0.2), 
        legend.title = element_blank(),
        legend.background = element_blank())
```

# Plot CI lengths

```{r, echo=FALSE}
set.seed(123)
wald_ci_width <- function (N, rate, alpha = 0.05){
  x <- rexp(N, rate = rate)
  x_bar <- mean(x)
  se <- x_bar / sqrt(N)
  ci <- x_bar + c(-1, 1)*qnorm(1 - (alpha / 2))*se
  width <- ci[2] - ci[1]
  return(width)
}

gamma_ci_width <- function(N, rate, alpha = 0.05){
  x <- rexp(N, rate = rate)
  x_bar <- mean(x)
  ci_rate <- qgamma(c(alpha / 2, 1 - (alpha / 2)), N, N) / x_bar
  ci_mean <- 1 / ci_rate
  diff <- ci_mean[2] - ci_mean[1]
  return(diff)
}

score_ci_width <- function(N, rate, alpha = 0.05) {
  x <- rexp(N, rate = rate)
  xbar <- mean(x)
  ci <- (1/rate)*(1 + c(-1, 1) * qnorm(1 - alpha / 2) / sqrt(N))
  diff <- ci[2] - ci[1]
  return(diff)
}

bootstrap_ci_width <- function(N, rate, alpha = 0.05){
  x <- rexp(N, rate = rate)
  x_bar <- mean(x)
  nb <- 1000
  bootstrap_samples <- sample(x, N * nb, replace = TRUE) %>%
    matrix(nrow = N, ncol = nb)
  means <- colMeans(bootstrap_samples)
  deltas <- means - x_bar
  deltas <- sort(deltas)
  ci <- x_bar - quantile(deltas, probs = c(alpha / 2, 1 - (alpha / 2)))
  diff <- ci[2] - ci[1]
  return(diff)
}

set.seed(123)
rate <- seq(0.01, 0.5, 0.01)

wald_width10 <- sapply(rate, FUN = wald_ci_width, N = 10)
gamma_width10 <- sapply(rate, FUN = gamma_ci_width, N = 10)
score_width10 <- sapply(rate, FUN = score_ci_width, N = 10)
bootstrap_width10 <- sapply(rate, FUN = bootstrap_ci_width, N = 10)

wald_width40 <- sapply(rate, FUN = wald_ci_width, N = 40)
gamma_width40 <- sapply(rate, FUN = gamma_ci_width, N = 40)
score_width40 <- sapply(rate, FUN = score_ci_width, N = 40)
bootstrap_width40 <- sapply(rate, FUN = bootstrap_ci_width, N = 40)

dat <- data.frame(lambda = rate, 
                  wald10 = wald_width10,
                  gamma10 = gamma_width10,
                  score10 = score_width10,
                  bootstrap10 = bootstrap_width10,
                  wald40 = wald_width40,
                  gamma40 = gamma_width40,
                  score40 = score_width40,
                  bootstrap40 = bootstrap_width40
                  )

dat <- abs(dat)

text <- c("Wald", "Gamma", "Score", "Bootstrap","Wald", "Gamma", "Score", "Bootstrap")
x <- c(0.05, 0.04, 0.04, 0.1, 0.04, 0.01, 0.05, 0.04)
y <- c(75, 190, 125, 50, 60, 10, 40, 85)
labels <- data.frame(text, x, y)

p <- ggplot(data = dat, aes(x = lambda)) +
  xlab("Lambda") +
  ylab("CI Width") +
  theme_bw()
  
p10 <- p +
  geom_line(aes(y = wald10), col = "dodgerblue") +
  geom_line(aes(y = gamma10), col = "burlywood") +
  geom_line(aes(y = score10), col = "indianred1") +
  geom_line(aes(y = bootstrap10), col = "seagreen") +
  geom_text(data = labels[1, ], aes(x, y, label = text), col = "dodgerblue", size = 3) +
  geom_text(data = labels[2, ], aes(x, y, label = text), col = "burlywood", size = 3) +
  geom_text(data = labels[3, ], aes(x, y, label = text), col = "indianred1", size = 3) +
  geom_text(data = labels[4, ], aes(x, y, label = text), col = "seagreen", size = 3) +
  ggtitle("CI Widths when N=10") +
  theme(legend.position="none") 

p40 <- p +
  geom_line(aes(y = wald40), color = "dodgerblue") +
  geom_line(aes(y = gamma40), color = "burlywood") +
  geom_line(aes(y = score40), color = "indianred1") +
  geom_line(aes(y = bootstrap40), color = "seagreen") +
  geom_text(data = labels[5, ], aes(x, y, label = text), col = "dodgerblue", size = 3) +
  geom_text(data = labels[6, ], aes(x, y, label = text), col = "burlywood", size = 3) +
  geom_text(data = labels[7, ], aes(x, y, label = text), col = "indianred1", size = 3) +
  geom_text(data = labels[8, ], aes(x, y, label = text), col = "seagreen", size = 3) +
  ggtitle("CI Widths when N=40") +
  theme(legend.position="none")

grid.arrange(p10, p40)

```

# Summary of Findings and Recommendations

## Estimators
- this estimator rocks, that one sucks

## Confidence intervals
- Gamma-based confidence interval is exact and coverage probability aligns with chosen $\alpha$ for all $n$, but tends to be large for small $n$ and $\lambda$
  - Only useful when the underlying distribution is known (not realistic)
- Wald- and score-based intervals perform similarly, with score providing better coverage at small $n$
- Bootstrap provides great flexibility, as does not require knowledge distribution underlying data
  - "Slowest" to provide coverage corresponding to $\alpha$, but could possibly be improved using more bootstrap samples (restricted here for computational time)

